# 5611_final_project
Our project shows success in using deep reinforcement learning which uses a neural network to simulate value function train control policy for robots in a virtual environment and do trajectory planning for robots in the real world. Moreover, we successfully applied the recent DRL algorithm Proximal Policy Optimization. In this project, We use DRL techniques to train a control policy with a 7-degree-of-freedom manipulator that performs the object-reaching task. We wrote our own mujoco module and penetrator for our model could run with the deep learning library.  
In thE algorithm part, we used a sim-to-real strategy, which trains a control policy in the virtual environment. In the virtual environment, we need to represent the robot using the robot model which has the same action space as the real robot, and an object as a target for the robot end-effector to reach. Moreover, We need to map the action space and observation space into the DRL algorithm which can used during policy training. During training, the target will randomly generate within Kinova reachable workspace. Therefore, we limit the target spawn location based on the manipulator workspace settings.  
difficulties(not sure yet)  
sketch of the project()  
The feedback we get from some of our peers most postitive, lots of them not familiar with Mujoco and Deep learning, but they show us some related robot movements link for reference.  
We mainly wrote a script for builiding a module in Mujoco and write a algorithm for connecting the Mujoco module movements and deep learning algorithms. In the future work, we might extend this project for multiple arms collaboration to pass the red ball. This might achieved by set the void "red ball: for arm1 and after arm1 move to the destination with redball, the arm 2 would grab the red ball to the new destination for the ball in sequence.  
