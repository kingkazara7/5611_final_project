# 5611_final_project
Our project shows success in using deep reinforcement learning, which uses a neural network to simulate value function train control policy for robots in a virtual environment and do trajectory planning for robots in the real world. Moreover, we successfully applied the recent DRL algorithm, Proximal Policy Optimization. In this project, We use DRL techniques to train a control policy with a 7-degree-of-freedom manipulator that performs the object-reaching task. We wrote our own Mujoco module, and the penetrator for our model could run with the deep learning library.  
In the algorithm part, we used a sim-to-real strategy, which trains a control policy in the virtual environment. In the virtual environment, we need to represent the robot using the robot model, which has the same action space as the real robot and an object as a target for the robot end-effector to reach. Moreover, We need to map the action space and observation space into the DRL algorithm, which can used during policy training. During training, the target will be randomly generated within Kinova's reachable workspace. Therefore, we limit the target spawn location based on the manipulator workspace settings.  
difficulties(not sure yet)  
sketch of the project()  
The feedback we get from some of our peers is most positive; lots of them are not familiar with Mujoco and Deep learning, but they show us some related robot movement links for reference.  
We mainly wrote a script for building a module in Mujoco and wrote an algorithm for connecting the Mujoco module movements and deep learning algorithms. In future work, we might extend this project for multiple arms collaboration to pass the red ball. This might achieved by setting the void "red ball: for arm1, and after arm1 moves to the destination with the red ball, arm2 two would grab the red ball to the new destination for the ball in sequence.  
